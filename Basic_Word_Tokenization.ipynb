{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1. Basic Word Tokenization"
      ],
      "metadata": {
        "id": "0ZIrC7FeBRfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install NLTK\n",
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9d84cX1Bueu",
        "outputId": "74bd195b-df26-403c-e227-845218e7f063"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "s_b-2fxSYTIV"
      },
      "outputs": [],
      "source": [
        "# Step 2: Import necessary libraries\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Download necessary NLTK resources (uncomment if needed)\n",
        "# nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Create a variable 'text'\n",
        "text = '''A paragraph is a series of related sentences developing a central idea, called the topic.\n",
        "          Try to think about paragraphs in terms of thematic unity: a paragraph is a sentence or a\n",
        "          group of sentences that supports one central, unified idea. Paragraphs add one idea at a\n",
        "          time to your broader argumentA paragraph is a series of related sentences developing a\n",
        "          central idea, called the topic. Try to think about paragraphs in terms of thematic unity:\n",
        "          a paragraph is a sentence or a group of sentences that supports one central, unified idea.\n",
        "          Paragraphs add one idea at a time to your broader argument '''"
      ],
      "metadata": {
        "id": "kdcyZY1l-rQc"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Tokenize the text into words\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Step 5: Print the list of tokens\n",
        "print(\"Tokens:\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbKq86j4-0Rc",
        "outputId": "8eb35e1f-827b-4874-cfe4-eba0213034a7"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['A', 'paragraph', 'is', 'a', 'series', 'of', 'related', 'sentences', 'developing', 'a', 'central', 'idea', ',', 'called', 'the', 'topic', '.', 'Try', 'to', 'think', 'about', 'paragraphs', 'in', 'terms', 'of', 'thematic', 'unity', ':', 'a', 'paragraph', 'is', 'a', 'sentence', 'or', 'a', 'group', 'of', 'sentences', 'that', 'supports', 'one', 'central', ',', 'unified', 'idea', '.', 'Paragraphs', 'add', 'one', 'idea', 'at', 'a', 'time', 'to', 'your', 'broader', 'argumentA', 'paragraph', 'is', 'a', 'series', 'of', 'related', 'sentences', 'developing', 'a', 'central', 'idea', ',', 'called', 'the', 'topic', '.', 'Try', 'to', 'think', 'about', 'paragraphs', 'in', 'terms', 'of', 'thematic', 'unity', ':', 'a', 'paragraph', 'is', 'a', 'sentence', 'or', 'a', 'group', 'of', 'sentences', 'that', 'supports', 'one', 'central', ',', 'unified', 'idea', '.', 'Paragraphs', 'add', 'one', 'idea', 'at', 'a', 'time', 'to', 'your', 'broader', 'argument']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Count the number of tokens\n",
        "print(\"Number of tokens:\", len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFLNFvpA_qzU",
        "outputId": "848fffd9-215c-421d-dd4b-7f1894df2d5b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens: 113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Identify the frequency of each token\n",
        "fdist = FreqDist(word.lower() for word in words)\n",
        "for token, frequency in fdist.items():\n",
        "    print(f\"{token}: {frequency}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67CpK2xZAINf",
        "outputId": "ef703308-674c-4aa4-f6ca-7c4014756163"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a: 13\n",
            "paragraph: 4\n",
            "is: 4\n",
            "series: 2\n",
            "of: 6\n",
            "related: 2\n",
            "sentences: 4\n",
            "developing: 2\n",
            "central: 4\n",
            "idea: 6\n",
            ",: 4\n",
            "called: 2\n",
            "the: 2\n",
            "topic: 2\n",
            ".: 4\n",
            "try: 2\n",
            "to: 4\n",
            "think: 2\n",
            "about: 2\n",
            "paragraphs: 4\n",
            "in: 2\n",
            "terms: 2\n",
            "thematic: 2\n",
            "unity: 2\n",
            ":: 2\n",
            "sentence: 2\n",
            "or: 2\n",
            "group: 2\n",
            "that: 2\n",
            "supports: 2\n",
            "one: 4\n",
            "unified: 2\n",
            "add: 2\n",
            "at: 2\n",
            "time: 2\n",
            "your: 2\n",
            "broader: 2\n",
            "argumenta: 1\n",
            "argument: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l2r-b_jTA-lu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}